from option import args
import model
import utils
import data.common as common

import torch
import numpy as np
import os
import glob
import cv2
import time
import psutil
from thop import profile
from torch.utils.tensorboard import SummaryWriter

# Additional imports for FLOPs calculation
PTFLOPS_AVAILABLE = False
FVCORE_AVAILABLE = False

try:
    import ptflops
    PTFLOPS_AVAILABLE = True
except (ImportError, AttributeError) as e:
    PTFLOPS_AVAILABLE = False

try:
    from fvcore.nn import flop_count
    FVCORE_AVAILABLE = True
except (ImportError, AttributeError) as e:
    FVCORE_AVAILABLE = False

device = torch.device('cpu' if args.cpu else 'cuda')


def test_model_performance(args, sr_model, sample_input=None):
    """æµ‹è¯•æ¨¡å‹æ€§èƒ½ï¼šå‚æ•°é‡ã€FLOPsã€å†…å­˜å ç”¨ã€å¤„ç†æ—¶é—´"""

    print("\n" + "="*60)
    print("æ¨¡å‹æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    print("="*60)

    # 1. æµ‹è¯•å‚æ•°é‡
    total_params = sum(p.numel() for p in sr_model.parameters())
    trainable_params = sum(p.numel() for p in sr_model.parameters() if p.requires_grad)

    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB")  # å‡è®¾float32

    # 2. æµ‹è¯•FLOPs
    print(f"\nğŸ”¢ FLOPsè®¡ç®—:")
    if sample_input is None:
        # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹è¾“å…¥
        if not args.cubic_input:
            sample_input = torch.randn(1, 3, 64, 64).to(device)
        else:
            sample_input = torch.randn(1, 3, 256, 256).to(device)

    flops = 0
    params = 0

    # Method 1: thop (original)
    try:
        flops_thop, params_thop = profile(sr_model, inputs=(sample_input,), verbose=False)
        flops += flops_thop
        params = params_thop
        print(f"  [thop] FLOPs: {flops_thop:,}")
        print(f"  [thop] FLOPs (G): {flops_thop / 1e9:.2f} G")
        print(f"  [thop] å‚æ•°é‡ (M): {params_thop / 1e6:.2f} M")
    except Exception as e:
        print(f"  [thop] FLOPsè®¡ç®—å¤±è´¥: {e}")

    # Method 2: ptflops
    flops_ptflops = None
    params_ptflops = None
    if PTFLOPS_AVAILABLE:
        try:
            # Get input shape
            input_shape = (3, sample_input.shape[2], sample_input.shape[3])

            # Use ptflops get_model_complexity_info with positional arguments
            macs, params_ptflops = ptflops.get_model_complexity_info(
                sr_model,
                input_shape,
                as_strings=False,  # Return numbers instead of strings
                input_constructor=lambda inputs: torch.randn(1, *input_shape).to(device),
                verbose=False
            )

            flops_ptflops = macs

            print(f"  [ptflops] FLOPs: {macs:,}")
            print(f"  [ptflops] FLOPs (G): {macs / 1e9:.2f} G")
            print(f"  [ptflops] å‚æ•°é‡ (M): {params_ptflops / 1e6:.2f} M")
        except Exception as e:
            print(f"  [ptflops] FLOPsè®¡ç®—å¤±è´¥: {e}")
            print(f"    é”™è¯¯è¯¦æƒ…: {str(e)}")
            flops_ptflops = None
            params_ptflops = None
    else:
        print(f"  [ptflops] åº“æœªå®‰è£…ï¼Œè·³è¿‡è®¡ç®—")

    # Method 3: fvcore
    total_flops = None
    if FVCORE_AVAILABLE:
        try:
            # fvcore's flop_count returns (total_flops, detailed_stats)
            result = flop_count(sr_model, (sample_input,))
            if isinstance(result, tuple):
                total_flops, flop_dict = result
            else:
                # If it's a dict directly
                flop_dict = result
                total_flops = sum(flop_dict.values())

            print(f"  [fvcore] FLOPs: {total_flops:,}")
            print(f"  [fvcore] FLOPs (G): {total_flops / 1e9:.2f} G")
            print(f"  [fvcore] è¯¦ç»†ç»Ÿè®¡:")
            if isinstance(flop_dict, dict):
                for op, count in flop_dict.items():
                    print(f"    {op}: {count:,}")
            else:
                print(f"    {flop_dict}")
        except Exception as e:
            print(f"  [fvcore] FLOPsè®¡ç®—å¤±è´¥: {e}")
            total_flops = None
    else:
        print(f"  [fvcore] åº“æœªå®‰è£…ï¼Œè·³è¿‡è®¡ç®—")

    # Comparison summary
    if PTFLOPS_AVAILABLE or FVCORE_AVAILABLE:
        print(f"\n  ğŸ“Š å„å·¥å…·è®¡ç®—ç»“æœå¯¹æ¯”:")
        print(f"    thop: {flops / 1e9:.2f} G FLOPs")
        if PTFLOPS_AVAILABLE and flops_ptflops is not None:
            print(f"    ptflops: {flops_ptflops / 1e9:.2f} G FLOPs")
        if FVCORE_AVAILABLE and total_flops is not None:
            print(f"    fvcore: {total_flops / 1e9:.2f} G FLOPs")

    # 3. æµ‹è¯•å†…å­˜å ç”¨
    print(f"\nğŸ’¾ å†…å­˜å ç”¨:")

    # GPUå†…å­˜å ç”¨
    if torch.cuda.is_available() and not args.cpu:
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

        # è®°å½•åˆå§‹GPUå†…å­˜
        if args.test_block:
            initial_memory = torch.cuda.memory_allocated()

            # æ‰§è¡Œå‰å‘ä¼ æ’­
            with torch.no_grad():
                _ = sr_model(sample_input)

            torch.cuda.synchronize()
            peak_memory = torch.cuda.max_memory_allocated()
            memory_used = peak_memory - initial_memory

            print(f"  GPUå†…å­˜å ç”¨: {memory_used / 1024 / 1024:.2f} MB")
        else:
            initial_memory = torch.cuda.memory_allocated()
            with torch.no_grad():
                _ = sr_model(sample_input)
            torch.cuda.synchronize()
            peak_memory = torch.cuda.max_memory_allocated()
            memory_used = peak_memory - initial_memory
            print(f"  GPUå†…å­˜å ç”¨: {memory_used / 1024 / 1024:.2f} MB")
    else:
        print(f"  CPUæ¨¡å¼ - GPUå†…å­˜: 0 MB")

    # CPUå†…å­˜å ç”¨
    process = psutil.Process()
    cpu_memory = process.memory_info().rss / 1024 / 1024
    print(f"  CPUå†…å­˜å ç”¨: {cpu_memory:.2f} MB")

    # 4. æµ‹è¯•å¤„ç†æ—¶é—´
    print(f"\nâ±ï¸  å¤„ç†æ—¶é—´æµ‹è¯•:")

    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(5):
            _ = sr_model(sample_input)

    if torch.cuda.is_available() and not args.cpu:
        torch.cuda.synchronize()

    # æ­£å¼æµ‹è¯•
    times = []
    for i in range(10):  # æµ‹è¯•10æ¬¡å–å¹³å‡
        if torch.cuda.is_available() and not args.cpu:
            torch.cuda.synchronize()
            start_time = time.time()

            with torch.no_grad():
                _ = sr_model(sample_input)

            torch.cuda.synchronize()
            end_time = time.time()
        else:
            start_time = time.time()
            with torch.no_grad():
                _ = sr_model(sample_input)
            end_time = time.time()

        elapsed = end_time - start_time
        times.append(elapsed)

        if i == 0:  # ç¬¬ä¸€æ¬¡å¯èƒ½æ¯”è¾ƒæ…¢ï¼Œè·³è¿‡
            continue

    avg_time = np.mean(times)
    std_time = np.std(times)
    min_time = np.min(times)
    max_time = np.max(times)

    print(f"  å¹³å‡å¤„ç†æ—¶é—´: {avg_time * 1000:.2f} ms")
    print(f"  æ ‡å‡†å·®: {std_time * 1000:.2f} ms")
    print(f"  æœ€å¿«æ—¶é—´: {min_time * 1000:.2f} ms")
    print(f"  æœ€æ…¢æ—¶é—´: {max_time * 1000:.2f} ms")

    # 5. ååé‡è®¡ç®—
    input_size = sample_input.shape
    if not args.cubic_input:
        output_size = (input_size[2] * args.scale[0], input_size[3] * args.scale[0])
    else:
        output_size = input_size[2:]

    pixels_processed = output_size[0] * output_size[1]
    throughput = pixels_processed / (avg_time * 1000)  # åƒç´ /æ¯«ç§’

    print(f"\nğŸ“ˆ ååé‡:")
    print(f"  è¾“å…¥å°ºå¯¸: {input_size[2]}x{input_size[3]}")
    print(f"  è¾“å‡ºå°ºå¯¸: {output_size[0]}x{output_size[1]}")
    print(f"  ååé‡: {throughput:.0f} åƒç´ /æ¯«ç§’")
    print(f"  FPS (ä¼°ç®—): {1000/avg_time:.1f}")

    print("="*60 + "\n")

    return {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'flops': flops if 'flops' in locals() else 0,
        'memory_used': memory_used if 'memory_used' in locals() else 0,
        'avg_time': avg_time,
        'fps': 1000/avg_time if avg_time > 0 else 0
    }


def deploy(args, sr_model):

    img_ext = '.tif'
    img_lists = glob.glob(os.path.join(args.dir_data, '*'+img_ext))

    if len(img_lists) == 0:
        print("Error: there are no images in given folder!")

    if not os.path.exists(args.dir_out):
        os.makedirs(args.dir_out)

    with torch.no_grad():
        for i in range(len(img_lists)):
            print("[%d/%d] %s" % (i+1, len(img_lists), img_lists[i]))
            lr_np = cv2.imread(img_lists[i], cv2.IMREAD_COLOR)
            lr_np = cv2.cvtColor(lr_np, cv2.COLOR_BGR2RGB)

            if args.cubic_input:
                lr_np = cv2.resize(lr_np, (lr_np.shape[0] * args.scale[0], lr_np.shape[1] * args.scale[0]),
                                interpolation=cv2.INTER_CUBIC)

            lr = common.np2Tensor([lr_np], args.rgb_range)[0].unsqueeze(0)

            if args.test_block:
                # test block-by-block

                b, c, h, w = lr.shape
                factor = args.scale[0]
                tp = args.patch_size
                if not args.cubic_input:
                    ip = tp // factor
                else:
                    ip = tp

                assert h >= ip and w >= ip, 'LR input must be larger than the training inputs'
                if not args.cubic_input:
                    sr = torch.zeros((b, c, h * factor, w * factor))
                else:
                    sr = torch.zeros((b, c, h, w))

                for iy in range(0, h, ip):

                    if iy + ip > h:
                        iy = h - ip
                    ty = factor * iy

                    for ix in range(0, w, ip):

                        if ix + ip > w:
                            ix = w - ip
                        tx = factor * ix

                        # forward-pass
                        lr_p = lr[:, :, iy:iy + ip, ix:ix + ip]
                        lr_p = lr_p.to(device)
                        sr_p = sr_model(lr_p)
                        sr[:, :, ty:ty + tp, tx:tx + tp] = sr_p

            else:

                lr = lr.to(device)
                sr = sr_model(lr)

            sr_np = np.array(sr.cpu().detach())
            sr_np = sr_np[0, :].transpose([1, 2, 0])
            lr_np = lr_np * args.rgb_range / 255.

            # Again back projection for the final fused result
            for bp_iter in range(args.back_projection_iters):
                sr_np = utils.back_projection(sr_np, lr_np, down_kernel='cubic',
                                           up_kernel='cubic', sf=args.scale[0], range=args.rgb_range)
            if args.rgb_range == 1:
                final_sr = np.clip(sr_np * 255, 0, args.rgb_range * 255)
            else:
                final_sr = np.clip(sr_np, 0, args.rgb_range)

            final_sr = final_sr.astype(np.uint8)
            final_sr = cv2.cvtColor(final_sr, cv2.COLOR_RGB2BGR)
            cv2.imwrite(os.path.join(args.dir_out, os.path.split(img_lists[i])[-1]), final_sr)
    #    # è®¡ç®—PSNR
    # folder_GT = '/root/autodl-tmp/TransENet/datasets/UCMerced-train/UCMerced-dataset/test/HR_x4'
    # folder_Gen = '/root/autodl-tmp/TransENet/experiment/results/UCMerced_UCMercedtest/x4'
    # img_ext = '.tif'
    # crop_border = 4  # same with scale
    # suffix = ''  # suffix for Gen images
    # test_Y = False  # True: test Y channel only; False: test RGB channels

    # PSNR_all = []
    # SSIM_all = []
    # print(f"\nè®¡ç®—PSNR...")

    # # è·å–GTå›¾åƒåˆ—è¡¨
    # gt_lists = sorted(glob.glob(os.path.join(folder_GT, '/*')))
    # for i, gt_path in enumerate(gt_lists):
    #     base_name = os.path.splitext(os.path.basename(gt_path))[0]
    #     gen_path = os.path.join(folder_Gen, base_name + img_ext)

    #     if not os.path.exists(gen_path):
    #         print(f"è­¦å‘Š: ç”Ÿæˆå›¾åƒä¸å­˜åœ¨: {gen_path}")
    #         continue

    #     # è¯»å–å›¾åƒ
    #     im_GT = cv2.imread(gt_path) / 255.
    #     im_Gen = cv2.imread(gen_path) / 255.

    #     # è£å‰ªè¾¹ç•Œ
    #     if crop_border == 0:
    #         cropped_GT = im_GT
    #         cropped_Gen = im_Gen
    #     else:
    #         if im_GT.ndim == 3:
    #             cropped_GT = im_GT[crop_border:-crop_border, crop_border:-crop_border, :]
    #             cropped_Gen = im_Gen[crop_border:-crop_border, crop_border:-crop_border, :]
    #         elif im_GT.ndim == 2:
    #             cropped_GT = im_GT[crop_border:-crop_border, crop_border:-crop_border]
    #             cropped_Gen = im_Gen[crop_border:-crop_border, crop_border:-crop_border]

    #     # è®¡ç®—PSNRå’ŒSSIM
    #     PSNR = calculate_rgb_psnr(cropped_GT * 255, cropped_Gen * 255)
    #     SSIM = calculate_ssim(cropped_GT * 255, cropped_Gen * 255)

    #     PSNR_all.append(PSNR)
    #     SSIM_all.append(SSIM)

    #     print(f"  {base_name:30s} - PSNR: {PSNR:.4f} dB, SSIM: {SSIM:.4f}")

    # avg_psnr = sum(PSNR_all) / len(PSNR_all)
    # avg_ssim = sum(SSIM_all) / len(SSIM_all)

    # print(f"\nå¹³å‡ç»“æœ: PSNR: {avg_psnr:.4f} dB, SSIM: {avg_ssim:.4f}")



if __name__ == '__main__':

    # args parameter setting
    # You can configure these paths via command line arguments or set them here
    args.pre_train = '/root/autodl-tmp/TransENet/experiment/test_dir/pair01_symunet_batch_lr0p0002_blk4-6_6-4_1xL1_schstep_w48/model/model_best.pt'
    
    # args.dir_data = '/root/autodl-tmp/TransENet/datasets/AID-train/AID-dataset/test/LR_x4'
    args.dir_data = '/root/autodl-tmp/TransENet/datasets/UCMerced-train/UCMerced-dataset/test/LR_x4'
    args.dir_out = '../experiment/results/466L1W32/x4'

    print("Configuration:")
    print(f"  Model: {args.model}")
    print(f"  Scale: {args.scale}")
    print(f"  Data dir: {args.dir_data}")
    print(f"  Output dir: {args.dir_out}")
    print(f"  Pre-trained model: {args.pre_train}")
    print("-" * 50)

    checkpoint = utils.checkpoint(args)
    sr_model = model.Model(args, checkpoint)
    sr_model.eval()

    # æ¨¡å‹æ€§èƒ½æµ‹è¯•
    test_model_performance(args, sr_model)

    # # analyse the params of the load model
    # pytorch_total_params = sum(p.numel() for p in sr_model.parameters())
    # print(pytorch_total_params)
    # pytorch_total_params2 = sum(p.numel() for p in sr_model.parameters() if p.requires_grad)
    # print(pytorch_total_params2)
    #
    # for name, p in sr_model.named_parameters():
    #     print(name)
    #     print(p.numel())
    #     print('========')

    # deploy(args, sr_model)