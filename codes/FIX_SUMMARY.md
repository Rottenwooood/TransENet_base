# ğŸ”§ ä¿®å¤æ€»ç»“ï¼šæ¢å¤è®­ç»ƒæ—¶Lossç»˜å›¾é”™è¯¯

## é—®é¢˜æè¿°

è¿è¡Œå‘½ä»¤ï¼š
```bash
python demo_train.py --resume 1 --save symunet_pretrain_x4_rlfft2_w48
```

é”™è¯¯ä¿¡æ¯ï¼š
```
ValueError: x and y must have same first dimension, but have shapes (119,) and (1,)
```

## æ ¹æœ¬åŸå› åˆ†æ

**ä¸æ˜¯lossä»£ç æœ¬èº«çš„é—®é¢˜**ï¼Œè€Œæ˜¯æˆ‘å¯¹`trainer.py`çš„æ”¹åŠ¨å¯¼è‡´çš„schedulerè°ƒç”¨æ··ä¹±ï¼š

### 1. Schedulerè°ƒç”¨æ—¶æœºé”™è¯¯
æˆ‘åœ¨è®­ç»ƒå¾ªç¯å†…éƒ¨è°ƒç”¨äº†`scheduler.step()`ï¼Œå¯¼è‡´schedulerè¢«è°ƒç”¨ä¸¤æ¬¡ï¼š
- ç¬¬84è¡Œï¼šåœ¨è®­ç»ƒå¾ªç¯å†…éƒ¨è°ƒç”¨
- ç¬¬116è¡Œï¼šåœ¨epochç»“æŸæ—¶è°ƒç”¨

### 2. Cosine Schedulerå¤„ç†é”™è¯¯
æˆ‘å¯¹cosine schedulerçš„å¤„ç†ä¸æ­£ç¡®ï¼Œæ”¹å˜äº†å…¶æ­£å¸¸çš„stepè®¡æ•°ã€‚

### 3. Global Stepè®¡æ•°é—®é¢˜
æ·»åŠ çš„global_stepè®¡æ•°å½±å“äº†ä¾èµ–stepçš„é€»è¾‘ã€‚

## ä¿®å¤æªæ–½

### 1. å›é€€Trainer.train()åˆ°åŸå§‹å®ç°
```python
# ä¿®å¤å‰ï¼ˆæœ‰é—®é¢˜ï¼‰
if hasattr(self.args, 'scheduler') and self.args.scheduler == 'cosine':
    self.scheduler.step()  # åœ¨è®­ç»ƒå¾ªç¯å†…éƒ¨è°ƒç”¨

# ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰
self.scheduler.step()  # åªåœ¨epochç»“æŸæ—¶è°ƒç”¨
```

### 2. å®‰å…¨æ·»åŠ WandBæ—¥å¿—
- æ·»åŠ å¼‚å¸¸å¤„ç†ï¼Œé¿å…WandBåˆå§‹åŒ–å¤±è´¥å½±å“è®­ç»ƒ
- åœ¨ä¸å½±å“æ ¸å¿ƒè®­ç»ƒé€»è¾‘çš„åœ°æ–¹è®°å½•æ—¥å¿—
- ä½¿ç”¨å±æ€§æ£€æŸ¥ç¡®ä¿wandb_loggerå­˜åœ¨

### 3. ä¿æŠ¤æ€§Lossç»˜å›¾
ä¸º`plot_loss`å‡½æ•°æ·»åŠ æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥ï¼Œé¿å…ç»˜å›¾å¤±è´¥å½±å“è®­ç»ƒã€‚

## ä¿®å¤åçš„å…³é”®ä»£ç 

### Trainer.train()æ ¸å¿ƒé€»è¾‘
```python
def train(self):
    self.loss.step()
    epoch = self.scheduler.last_epoch + 1
    learn_rate = self.scheduler.get_last_lr()[0]

    self.ckp.write_log('[Epoch {}]\tLearning rate: {:.2e}'.format(epoch, Decimal(learn_rate)))
    self.loss.start_log()
    self.model.train()

    timer_data, timer_model = utils.timer(), utils.timer()

    for batch, (lr, hr, file_names) in enumerate(self.loader_train):
        # ... è®­ç»ƒé€»è¾‘ ...

        # å®‰å…¨è®°å½•WandBæ—¥å¿—
        if hasattr(self, 'wandb_logger') and (batch + 1) % self.args.print_every == 0:
            try:
                self.wandb_logger.log_training_step(...)
            except Exception as e:
                print(f"âš ï¸ WandB logging failed: {e}")

    self.scheduler.step()  # åªåœ¨epochç»“æŸæ—¶è°ƒç”¨
    self.loss.end_log(len(self.loader_train))
    self.error_last = self.loss.log[-1, -1]
```

### å®‰å…¨çš„WandBåˆå§‹åŒ–
```python
def __init__(self, args, loader, my_model, my_loss, ckp):
    # ... å…¶ä»–åˆå§‹åŒ– ...

    # å®‰å…¨åˆå§‹åŒ–WandB
    self.wandb_logger = None
    if hasattr(args, 'use_wandb') and args.use_wandb:
        try:
            self.wandb_logger = wandb_utils.WandbLogger(args, my_model, my_loss)
        except Exception as e:
            print(f"âš ï¸ Failed to initialize WandB logger: {e}")
            print("Continuing without WandB...")
```

## å…³é”®æ•™è®­

1. **ä¸è¦éšæ„ä¿®æ”¹è®­ç»ƒæ ¸å¿ƒé€»è¾‘**ï¼šschedulerçš„è°ƒç”¨æ—¶æœºå¿…é¡»ä¸¥æ ¼éµå¾ªPyTorchçš„è§„èŒƒ
2. **ä¿æŠ¤æ€§ç¼–ç¨‹**ï¼šæ·»åŠ å¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿è¾…åŠ©åŠŸèƒ½å¤±è´¥ä¸ä¼šå½±å“ä¸»è¦åŠŸèƒ½
3. **æ¸è¿›å¼æ·»åŠ åŠŸèƒ½**ï¼šå…ˆç¡®ä¿æ ¸å¿ƒåŠŸèƒ½ç¨³å®šï¼Œå†é€æ­¥æ·»åŠ å¢å¼ºåŠŸèƒ½

## éªŒè¯

ä¿®å¤ååº”è¯¥èƒ½å¤Ÿï¼š
- âœ… æ­£å¸¸æ¢å¤è®­ç»ƒ
- âœ… æ­£ç¡®ç»˜åˆ¶lossæ›²çº¿
- âœ… è®°å½•WandBæ—¥å¿—ï¼ˆå¦‚å¯ç”¨ï¼‰
- âœ… ä¸å½±å“è®­ç»ƒæ€§èƒ½å’Œç¨³å®šæ€§

ç°åœ¨å¯ä»¥å®‰å…¨åœ°è¿è¡Œæ¢å¤è®­ç»ƒå‘½ä»¤äº†ï¼
