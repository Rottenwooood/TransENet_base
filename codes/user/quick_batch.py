#!/usr/bin/env python3
"""
å¿«é€Ÿæ‰¹é‡è®­ç»ƒè„šæœ¬ - é¢„è®¾å‡ ç§å¸¸ç”¨çš„å®éªŒé…ç½®

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€äº›é¢„è®¾çš„å®éªŒé…ç½®ï¼Œæ–¹ä¾¿å¿«é€Ÿå¼€å§‹ï¼š
1. å­¦ä¹ ç‡å¯¹æ¯”å®éªŒ
2. ä¼˜åŒ–å™¨å¯¹æ¯”å®éªŒ
3. æ¨¡å‹å®½åº¦å¯¹æ¯”å®éªŒ
4. æŸå¤±å‡½æ•°å¯¹æ¯”å®éªŒ

ä½¿ç”¨æ–¹æ³•:
python quick_batch.py --preset lr_comparison
python quick_batch.py --preset all
"""

import os
import sys
import argparse
import subprocess
from pathlib import Path


class QuickBatchTrainer:
    def __init__(self):
        self.base_dir = Path.cwd()
        self.experiment_dir = self.base_dir.parent / "experiment"
        self.experiment_dir.mkdir(exist_ok=True)

    def get_presets(self):
        """è·å–é¢„è®¾é…ç½®"""
        return {
            "lr_comparison": {
                "name": "å­¦ä¹ ç‡å¯¹æ¯”å®éªŒ",
                "description": "æ¯”è¾ƒä¸åŒå­¦ä¹ ç‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“",
                "experiments": [
                    {
                        "name": "lr_1e-4",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "cosine",
                            "lr": 1e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1+0.005*FFT",
                            "symunet_pretrain_width": 48,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "lr_1e-4"
                        }
                    },
                    {
                        "name": "lr_2e-4",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "cosine",
                            "lr": 2e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1+0.005*FFT",
                            "symunet_pretrain_width": 48,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "lr_2e-4"
                        }
                    },
                    {
                        "name": "lr_5e-4",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "cosine",
                            "lr": 5e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1+0.005*FFT",
                            "symunet_pretrain_width": 48,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "lr_5e-4"
                        }
                    }
                ]
            },
            "optimizer_comparison": {
                "name": "ä¼˜åŒ–å™¨å¯¹æ¯”å®éªŒ",
                "description": "æ¯”è¾ƒAdamWå’ŒAdamä¼˜åŒ–å™¨çš„æ€§èƒ½",
                "experiments": [
                    {
                        "name": "adamw_cosine",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "cosine",
                            "lr": 2e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1+0.005*FFT",
                            "symunet_pretrain_width": 48,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "adamw_cosine"
                        }
                    },
                    {
                        "name": "adam_cosine",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAM",
                            "scheduler": "cosine",
                            "lr": 2e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1+0.005*FFT",
                            "symunet_pretrain_width": 48,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "adam_cosine"
                        }
                    },
                    {
                        "name": "adamw_step",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "step",
                            "lr": 2e-4,
                            "lr_decay": 100,
                            "loss": "1*L1+0.005*FFT",
                            "symunet_pretrain_width": 48,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "adamw_step"
                        }
                    }
                ]
            },
            "width_comparison": {
                "name": "æ¨¡å‹å®½åº¦å¯¹æ¯”å®éªŒ",
                "description": "æ¯”è¾ƒä¸åŒæ¨¡å‹å®½åº¦å¯¹æ€§èƒ½çš„å½±å“",
                "experiments": [
                    {
                        "name": "width_32",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "cosine",
                            "lr": 2e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1+0.005*FFT",
                            "symunet_pretrain_width": 32,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "width_32"
                        }
                    },
                    {
                        "name": "width_48",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "cosine",
                            "lr": 2e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1+0.005*FFT",
                            "symunet_pretrain_width": 48,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "width_48"
                        }
                    },
                    {
                        "name": "width_64",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "cosine",
                            "lr": 2e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1+0.005*FFT",
                            "symunet_pretrain_width": 64,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "width_64"
                        }
                    }
                ]
            },
            "loss_comparison": {
                "name": "æŸå¤±å‡½æ•°å¯¹æ¯”å®éªŒ",
                "description": "æ¯”è¾ƒä¸åŒæŸå¤±å‡½æ•°ç»„åˆçš„æ•ˆæœ",
                "experiments": [
                    {
                        "name": "loss_l1_only",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "cosine",
                            "lr": 2e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1",
                            "symunet_pretrain_width": 48,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "loss_l1_only"
                        }
                    },
                    {
                        "name": "loss_l1_fft_005",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "cosine",
                            "lr": 2e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1+0.005*FFT",
                            "symunet_pretrain_width": 48,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "loss_l1_fft_005"
                        }
                    },
                    {
                        "name": "loss_l1_fft_01",
                        "config": {
                            "model": "SYMUNET_PRETRAIN",
                            "dataset": "UCMerced",
                            "scale": 4,
                            "epochs": 200,
                            "batch_size": 4,
                            "optimizer": "ADAMW",
                            "scheduler": "cosine",
                            "lr": 2e-4,
                            "cosine_t_max": 200,
                            "loss": "1*L1+0.01*FFT",
                            "symunet_pretrain_width": 48,
                            "symunet_pretrain_enc_blk_nums": "4,6,6",
                            "symunet_pretrain_dec_blk_nums": "6,6,4",
                            "save_every_n_steps": 50,
                            "save": "loss_l1_fft_01"
                        }
                    }
                ]
            }
        }

    def list_presets(self):
        """åˆ—å‡ºæ‰€æœ‰é¢„è®¾é…ç½®"""
        presets = self.get_presets()

        print("ğŸš€ Available Presets:")
        print("=" * 60)

        for key, preset in presets.items():
            print(f"\nğŸ“¦ {key}")
            print(f"   Name: {preset['name']}")
            print(f"   Description: {preset['description']}")
            print(f"   Experiments: {len(preset['experiments'])}")

    def run_preset(self, preset_key: str, use_wandb: bool = True, dry_run: bool = False):
        """è¿è¡Œé¢„è®¾å®éªŒ"""
        presets = self.get_presets()

        if preset_key not in presets:
            print(f"âŒ Preset '{preset_key}' not found")
            return

        preset = presets[preset_key]
        experiments = preset['experiments']

        print(f"\nğŸ§ª Running preset: {preset['name']}")
        print(f"ğŸ“‹ Description: {preset['description']}")
        print(f"ğŸ”¢ Experiments: {len(experiments)}")

        # æ˜¾ç¤ºå®éªŒåˆ—è¡¨
        print(f"\nğŸ“‹ Experiment List:")
        for i, exp in enumerate(experiments, 1):
            print(f"   {i}. {exp['name']}")

        # è¯¢é—®ç¡®è®¤
        if not dry_run:
            response = input(f"\nDo you want to run {len(experiments)} experiments? (y/N): ")
            if response.lower() not in ['y', 'yes']:
                print("âŒ Cancelled")
                return

        # æ‰§è¡Œå®éªŒ
        for i, experiment in enumerate(experiments, 1):
            exp_name = experiment['name']
            config = experiment['config']

            # æ·»åŠ WandBé…ç½®
            if use_wandb:
                config['use_wandb'] = True
                config['wandb_project'] = f"SymUNet-{preset_key}"
                config['wandb_name'] = exp_name

            print(f"\n{'='*60}")
            print(f"ğŸ”¬ Experiment {i}/{len(experiments)}: {exp_name}")
            print(f"{'='*60}")

            # æ˜¾ç¤ºé…ç½®
            print(f"ğŸ“‹ Configuration:")
            for key, value in config.items():
                print(f"   {key}: {value}")

            if dry_run:
                print(f"ğŸ” Dry run - would run:")
                cmd = self.build_command(config)
                print(f"   Command: {cmd}")
                continue

            # æ‰§è¡Œè®­ç»ƒ
            try:
                cmd = self.build_command(config)
                print(f"\nğŸš€ Starting training...")

                result = subprocess.run(cmd, shell=True, check=True)
                print(f"âœ… Experiment {exp_name} completed successfully")

            except subprocess.CalledProcessError as e:
                print(f"âŒ Experiment {exp_name} failed: {e}")
            except KeyboardInterrupt:
                print(f"âš ï¸ Training interrupted by user")
                break

        print(f"\nğŸ‰ Preset '{preset_key}' completed!")

    def build_command(self, config: dict) -> str:
        """æ„å»ºè®­ç»ƒå‘½ä»¤"""
        cmd_parts = ["python", "train_enhanced.py"]

        for key, value in config.items():
            if isinstance(value, bool):
                if value:
                    cmd_parts.append(f"--{key}")
            else:
                cmd_parts.append(f"--{key}")
                cmd_parts.append(str(value))

        return " ".join(cmd_parts)

    def run_all_presets(self, use_wandb: bool = True, dry_run: bool = False):
        """è¿è¡Œæ‰€æœ‰é¢„è®¾å®éªŒ"""
        presets = self.get_presets()

        print(f"ğŸš€ Running ALL presets ({len(presets)} total)")
        print(f"âš ï¸ This will take approximately {len(presets) * 3:.0f} hours")

        if not dry_run:
            response = input("Do you want to continue? (y/N): ")
            if response.lower() not in ['y', 'yes']:
                print("âŒ Cancelled")
                return

        for preset_key in presets.keys():
            print(f"\n{'#'*80}")
            print(f"ğŸ¯ Processing preset: {preset_key}")
            print(f"{'#'*80}")

            self.run_preset(preset_key, use_wandb, dry_run)


def main():
    parser = argparse.ArgumentParser(description="å¿«é€Ÿæ‰¹é‡è®­ç»ƒè„šæœ¬")
    parser.add_argument("--preset", type=str, help="è¦è¿è¡Œçš„é¢„è®¾é…ç½®")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨é¢„è®¾")
    parser.add_argument("--all", action="store_true", help="è¿è¡Œæ‰€æœ‰é¢„è®¾")
    parser.add_argument("--no-wandb", action="store_true", help="ç¦ç”¨WandB")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œï¼ˆä¸å®é™…æ‰§è¡Œï¼‰")

    args = parser.parse_args()

    trainer = QuickBatchTrainer()

    if args.list:
        trainer.list_presets()
    elif args.all:
        trainer.run_all_presets(use_wandb=not args.no_wandb, dry_run=args.dry_run)
    elif args.preset:
        trainer.run_preset(args.preset, use_wandb=not args.no_wandb, dry_run=args.dry_run)
    else:
        print("ğŸš€ Quick Batch Training")
        print("\nUsage:")
        print("  python quick_batch.py --list                    # åˆ—å‡ºæ‰€æœ‰é¢„è®¾")
        print("  python quick_batch.py --preset lr_comparison     # è¿è¡Œå­¦ä¹ ç‡å¯¹æ¯”å®éªŒ")
        print("  python quick_batch.py --all                     # è¿è¡Œæ‰€æœ‰é¢„è®¾å®éªŒ")
        print("  python quick_batch.py --preset optimizer_comparison --dry-run  # è¯•è¿è¡Œ")
        print("\nAvailable presets:")
        trainer.list_presets()


if __name__ == "__main__":
    main()
